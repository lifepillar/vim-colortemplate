*libtinytest.txt*	For Vim version 9.1	Last change: 2025 Jan 14

Note: libtinytest can work with Vim 9.0 or later, but depending on the patch
level, you may encounter issues. Vim 9.1.1013 or later is recommended.

Tiny Test is a testing and benchmarking library for Vim 9 script developers.
A test is just a function without arguments and return value whose name starts
with `Test`. Typically, tests contain Vim assertions (see
|assert-functions-details|). For instance:
>
	vim9script

	import 'libtinytest.vim' as tt

	def Test_OnePlusOneIsTwo()
	  assert_equal(2, 1 + 1)
	enddef

	var test_results = tt.Run()
<
The output appears in a new scratch buffer by default.

Tests may also be used for benchmarking. See |libtinytest.AssertBenchmark()|.

The tests may be read from a different file using |libtinytest.Import|. For
example, this is how you may import a file `tests.vim` in the same directory
as the main test file:
>
	vim9script

	import 'libtinytest.vim' as tt
	tt.Import($"{expand('<sfile>:h')}/tests.vim")

	const success = tt.Run()
<
Using |:source| is also possible, but |libtinytest.Import| avoids running the
tests twice if the imported file contains a `Run()` statement.

It is possible to automatically execute a function before or after each test.
The functions must be assigned to |libtinytest.Setup| and
|libtinytest.Teardown|, respectively. For example:
>
	vim9script
	import 'libtinytest.vim' as tt

	tt.Setup = () => {
	  echo $'Before starting test {tt.done + 1}'
	}
	tt.Teardown = () => {
	  echo $'After finishing test {tt.done}'
	}

	# Tests here
	tt.Run()
<
Obviously, all the names that are used in tests must be visible to the tests.
Names that are exported by the script to be tested can be imported in the test
script. Sometimes, however, it is convenient to be able to test the private,
not exported, components of a script. To do so, the tests must be inlined in
the script itself. A variation of the following structure can serve the
purpose:
>
	vim9script

	# Here goes the code to be tested

	if !get(g:, 'test_mode', false)
	  finish
	endif

	import 'libtinytest.vim' as tt

	# Write the tests here
<
Then, to run the tests just set `g:test_mode` to `true` and source the script.

Variables ~

					*libtinytest.done*
done
		Counter of the number of tests that have been executed.

					*libtinytest.fail*
fail
		Counter of the number of tests that have failed.

					*libtinytest.options*
options
		User settings. See |libtinytest-options| for details.

					*libtinytest.Setup*
Setup
		This variable may be set to a function with no arguments and
		no return value, which is executed before each test.

					*libtinytest.Teardown*
Teardown
		This variable may be set to a function with no arguments and
		no return value, which is executed after each test. The
		function is executed no matter whether a test succeeds or
		fails.

					*libtinytest-functions*
Functions ~

					*libtinytest.AssertApprox()*
AssertApprox({expected}, {num}[, {rtol}, {atol}, {msg}])
		Check that {num} is approximately equal to the {expected}
		value, where "approximately" means within the relative
		tolerance {rtol} or within the absolute tolerance {atol} (or
		within both). If {msg} is provided, it is prefixed to the
		error message if the test fails.

		The default relative threshold is 0.001 (1‰ tolerance), and
		the default absolute tolerance is 0.0. So, by default only the
		relative threshold matters.

		For example:
>
		AssertApprox(1.0, 1.00001)          # OK
		AssertApprox(1.0, 1.01)             # Fails
		AssertApprox(10.0, 11.0, 0.1)       # Relative tolerance OK
		AssertApprox(10.0, 11.0, 0.0, 1.0)  # Absolute tolerance OK
		AssertApprox(10.0, 11.0, 0.05)      # Fails
		AssertApprox(10.0, 11.0, 0.05, 0.9) # Fails
<
					*libtinytest.AssertBenchmark()*
AssertBenchmark({func}[, {desc}, {opts}])
		Measure the time needed to run {func}, which is a function
		with no arguments and no return value. {desc} is a short
		description of the benchmark, used to identify the benchmark
		in the output.

		{opts} is a dictionary of options to configure how the
		benchmark is performed. The following keys can be used:

		- "repeat": the number of measurements to collect. A larger
		  number of repetitions should reduce the uncertainty on the
		  result (but that may not be true in practice). The default
		  is to collect one measurement.

		- "loop": the number of times {func} should be executed in
		  each measurement. Generally speaking, the faster the
		  function to benchmark the higher this parameter should be.
		  When this is omitted or set to 0, the number of iterations
		  is determined automatically.

		- "severity": a dictionary whose keys describe a benchmark
		  outcome and whose values are floats denoting the minimum
		  threshold for the corresponding outcome. Times are in
		  milliseconds. This can be used to assign a qualitative
		  evaluation to the benchmark result. See the example below.

		This is how you would benchmark |sleep|:
>
		def Sleep()
		  sleep 1m
		enddef

		AssertBenchmark(Sleep, 'Sleep', {
		  repeat: 5,
		  severity: {
		    '[too fast!]': 0.0,
		    '[ok]':        1.0,
		    '[too slow!]': 1.3,
		  }
		})
<
		The output may look like this:
>
		[ok] Sleep: 1.274ms±0.001 (best: 1.272ms) [5 runs, 200 loops per run]
<
		The result shows the average execution time across
		5 measurements (1.274ms) and the standard deviation of the
		measurements (0.001ms). Each measurement was computed by
		executing `Sleep()` 200 times and by dividing the total
		elapsed time by 200.

		The result has been labeled with `[ok]` because the average
		execution time was larger than the severity threshold
		associated with the `[ok]` label, but strictly lower than the
		threshold associated with the `[too slow!]` label.

AssertFails({func}, {errmsg}[, {msg}])	*libtinytest.AssertFails()*
		Check that {func} throws an error that contains the string
		{errmsg}. {msg} is an optional message shown when the test
		fails. {func} must be a function or lambda with no arguments
		and no return value. For example this assertion succeeds:
>
		AssertFails(() => {
		    throw 'This is a fatal error'
		}, 'fatal')
<
		This assertion fails because the function does not throw any
		error:
>
		AssertFails(() => {
		    return
		}, 'err')
<
		And this assertion fails because the thrown error does not
		contain the expected text:
>
		AssertFails(() => {
		    throw 'This is a silly error'
		}, 'fatal')
<
Import({path})				*libtinytest.Import()*
		Import tests from {path}.

Round({num}, {digits})			*libtinytest.Round()*
		Round the given number to the specified number of {digits}.
		For example:
>
		echo Round(4.54898, 2)  # 4.55
<
Run([{pattern}, {opts}])		*libtinytest.Run()*
		Run all the tests. If {pattern} is given, run only the tests
		matching {pattern}. For instance, to run only the tests
		containing `ABC`, you may write:
>
		var test_results = Run('ABC')
<
		This will run `Test_ABC_Foo()` and `TestABCBar()`, but not
		`Test_XYZ()`.

		{opts} is a dictionary of options that override
		|libtinytest-options| for a specific run. For instance:
>
		Run('', {oksymbol: 'OK!'})
<
		Returns a list with test result objects. Each object has the
		following fields:

		- `test`: the name of the test.
		- `elapsed`: the duration of the test in milliseconds.
		- `errors`: a list of error strings produced by the test.
		- `benchmarks`: a list of benchmark results if any benchmarks
		  were run inside the test.

		A benchmark result object has the following fields:

		- `description`: the benchmark description.
		- `measurements`: the list of measurements in milliseconds.
		- `loop`: the number of iterations in each measurement.
		- `severity`: the severity labels and correponding thresholds.

		See |libtinytest.AssertBenchmark()| for details.

		For instance:
>
		var test_results = libtinytest.Run('', {quiet: true})

		for result in test_results
		  echo $'{result.test} took {result.elapsed}ms'

		  if result.Ok()
		    echo 'The test was successful'
		  else
		    echo 'There were errors:' result.errors
		  endif
		endfor
<
					*libtinytest-options*
Options ~

For each of the following script variables a corresponding global variable
called `g:libtinytest_<option>` may also be defined. For instance, the
highlight can be controlled by setting |libtinytest.options.highlight|, but
also |g:libtinytest_highlight|. Setting the script variables is preferrable
because they become effective on the next test run: global variables, instead,
must be defined before libtinytest is loaded.

					*libtinytest.options.failedsymbol*
String to label failed tests.
>
	libtinytest.options.failedsymbol = '✘'
<
					*libtinytest.options.highlight*
Flag to enable or disable syntax highlighting in the output.
>
	libtinytest.options.highlight = true
<
					*libtinytest.options.oksymbol*
String to label successful tests.
>
	libtinytest.options.oksymbol = '✔︎'
<
					*libtinytest.options.quiet*
By default, test results are displayed in a new scratch buffer. Set this to
|true| to suppress any output. Running in quiet mode is useful in case you
want to process the test results yourself. See |libtinytest.Run()|.
>
	libtinytest.options.quiet = false
<
 vim:tw=78:ts=8:noet:ft=help:norl:
